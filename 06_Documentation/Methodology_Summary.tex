%==== KNITR ===================================================================%



%==== START ===================================================================%

\documentclass{report}\usepackage[]{graphicx}\usepackage[]{xcolor}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}

\usepackage[left=2cm, right=2cm, top=1cm, bottom=2cm]{geometry}

% Font.


% Main packages.
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{booktabs} 
\usepackage{rotating} 
\usepackage{lmodern}

% Citations.
\usepackage{natbib}
\setcitestyle{authoryear,open={(},close={)}} %Citation-related commands

%% Testing different layout.
\usepackage{geometry}
\usepackage{xcolor}

% Required for Table.


%%

% \title{Master Thesis - Research Proposal}
% \author{Tristan Leiter}
% \date{\today}

%==== DOCUMENT START ==========================================================%

\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}

\begin{titlepage}
    \newgeometry{top=3cm, bottom=3cm, left=2.5cm, right=2.5cm}
    \centering
    
    % --- University Placeholder ---
    % \includegraphics[width=0.4\textwidth]{logo.png} \par
    {\scshape\LARGE Vienna University of Economics and Business \par}
    \vspace{1cm}
    {\scshape\Large OeNB ILAB \par}
    
    \vspace{2.5cm}
    
    % --- Title Section ---
    \hrule height 2pt
    \vspace{0.5cm}
    { \huge \bfseries OeNB ILAB: \\[0.3cm] Methodology Summary \par}
    \vspace{0.5cm}
    \hrule height 2pt
    
    \vspace{2cm}
    
    % --- Author Section ---
    \Large
    \textbf{Author:} \\
    Tristan \textsc{Leiter}
        Anastasia \textsc{Pryshchepa}

    
    \vspace{1.5cm}
    
    % --- Metadata ---
    \normalsize
    \textbf{Submission Date:} \\
    \today

    \vfill
    
\end{titlepage}
\restoregeometry

% \maketitle

%==== ABSTRACT ================================================================%

% \begin{abstract}
% This report demonstrates the integration of R code and its output within a LaTeX document using Sweave. It covers the basic structure of a report, including a summary, chapters with subchapters, and a bibliography.
% \end{abstract}

%==== Table of content ========================================================%

\tableofcontents
\newpage

%==== Chapter 1: Multi-variate stratified sampling ============================%

\chapter{Data Processing}

\section{Preprocessing and data splitting}

\subsection{Preprocessing}

Based on the instructions from the client, we filter the balance sheet data based on the following constraints to remove noisy observations. To account for floating-point inconsistencies, we add the condition that the difference must be greater than 2000 (as per client instructions).

\begin{enumerate}
    \item $f_{10} \ge 0$
    \item $(f_2 + f_3) \le f_1$
    \item $(f_4 + f_5) \le f_3$
    \item $(f_6 + f_{11}) \le f_1$
\end{enumerate}

\subsection{Data splitting}

We utilize a custom function, \texttt{MVstratifiedsampling}, to perform a stratified split at the firm level rather than the observation level. This ensures that all records for a specific firm ID reside in the same partition.

\begin{enumerate}
  \item \textbf{Aggregate to Firm Level} \\
  The dataset is first grouped by unique identifier (\texttt{id}). We summarize the data to create a single profile per firm, extracting the maximum default status ($y$) and the business sector.

  \item \textbf{Create Stratification Key} \\
  We generate a combined key for each unique firm by interacting the stratification variables (e.g., \texttt{Sector} and \texttt{Target}).
  \begin{itemize}
    \item This creates composite levels (e.g., \texttt{"Energy.1"}, \texttt{"Retail.0"}) used to balance the distribution of the target variable across sectors.
  \end{itemize}

  \item \textbf{Partition Unique IDs} \\
  Using the \texttt{createDataPartition} algorithm from the \textit{caret} package, we split the unique firm IDs based on the stratification key.
  \begin{itemize}
    \item The default split creates a training set containing 70\% of the firms and a test set containing the remaining 30\%.
  \end{itemize}

  \item \textbf{Retrieve Observation Data} \\
  Finally, we filter the original dataset to reconstruct the Training and Test sets based on the selected lists of IDs. This preserves the original data structure without requiring column removal.
\end{enumerate}

\section{Feature engineering}

\subsection{Standardization (Size Normalization)}
Financial data often exhibits a "size effect," where absolute magnitude (e.g., total sales or debt in Euros) overshadows financial performance. To ensure comparability between large and small firms while preserving the risk signal associated with company size, we apply a two-step logic:

\begin{enumerate}
    \item \textbf{Ratio Creation:} We scale absolute financial figures (e.g., Debt, Cash, EBIT) against \textbf{Total Assets}. This converts raw figures into structural ratios (e.g., $\text{EBIT} \rightarrow \text{ROA}$), isolating efficiency from magnitude.
    \item \textbf{Size Retention:} We explicitly retain the \textbf{Total Assets} feature. In credit risk, size itself is often a predictor of stability (e.g., larger firms having better capital access). By keeping it as a feature, we allow the model to learn these non-linear interactions between "Efficiency" (ratios) and "Scale" (assets).
\end{enumerate}

\subsection{Quantile Transformation (Probability Integral Transform)}
After size normalization, both the financial ratios and the size feature typically remain highly skewed with heavy tails (non-Gaussian). We apply a \textbf{Quantile Transformation} (Rank-Gauss) using the Probability Integral Transform (PIT) to all continuous features.

To prevent data leakage and preserve macro-economic signals (e.g., a global downturn increasing leverage ratios across the board), we utilize a \textbf{Frozen Reference Approach}:
\begin{enumerate}
    \item \textbf{Training Phase:} We compute the ranks and Empirical Cumulative Distribution Function (ECDF) on the training set to establish a "Through-the-Cycle" reference distribution.
    \item \textbf{Testing Phase (Walking Forward):} We map future observations (Test set) onto this fixed training ECDF. This ensures that if the economy deteriorates (shifting the test distribution right), the transformed Z-scores reflect this increased risk relative to the training baseline, rather than normalizing it away.
\end{enumerate}

The transformation chain for any continuous variable $x$ (whether a calculated ratio or raw Total Assets) is:

$$ 
x_{final} = \Phi^{-1}\left( ECDF_{train}(x_{input}) \right) \quad \text{where} \quad x_{final} \sim N(0,1) 
$$

Where $x_{input}$ is either the calculated ratio ($\frac{x_{raw}}{\text{Assets}}$) or the raw Size variable itself.

% \subsection{Extreme Values}



%==== Chapter 2: Problem Description ==========================================%

\chapter{Modelling}

\section{Model Selection}

Selection is done by the AuC-ROC measure as per client instructions. \\

\section{Hyperparameter Tuning}

\subsection{Discrete Grid Search}

\subsection{Random Grid Search}

\subsection{Bayesian Optimization}

\section{GLMs}

\subsection{Logistic Regression}

\subsection{Regularized GLMs}

\section{Decision Trees}

\subsection{Random Forest}

We use a Random Forest classifier to predict firm default events. The Random Forest
combines a large number of classification trees, each trained on a different bootstrap
sample of the training data. At each split in a tree, only a random subset of predictors
is considered. This randomness reduces correlation across trees and improves predictive
performance.Each tree produces a predicted default probability, and final predictions are obtained
by aggregating the results across all trees.

\subsubsection{Hyperparameter Specification}

The Random Forest is estimated with the following hyperparameter settings:

\begin{itemize}
    \item \textbf{Number of trees ($B$):}  
    The number of trees is set to $B = 500$. A large number of trees stabilizes the
    ensemble predictions and reduces Monte Carlo noise. Increasing $B$ does not lead
    to overfitting.

    \item \textbf{Number of predictors per split ($m$):}  
    At each split, the number of candidate predictors is set to
    $m = \lfloor \sqrt{p} \rfloor$, where $p$ denotes the total number of predictors.
    This is the standard choice for classification tasks and helps decorrelate trees.

    \item \textbf{Minimum terminal node size:}  
    The minimum node size is set to 1, allowing trees to grow fully. Individual trees
    are therefore highly flexible (low bias), while variance is reduced through
    aggregation across the forest.
\end{itemize}

\subsubsection{Algorithm}

Let the training sample be
\[
\mathcal{Z} = \{(x_i, y_i)\}_{i=1}^N,
\]
where $x_i$ is a vector of firm-level predictors and $y_i \in \{0,1\}$ indicates default
status.

The Random Forest algorithm proceeds as follows:

\begin{enumerate}
    \item \textbf{Bootstrap sampling} \\
    For each tree $b = 1, \dots, B$, a bootstrap sample of size $N$ is drawn with
    replacement from the training data.

    \item \textbf{Tree growing} \\
    A classification tree is grown on each bootstrap sample. At each split:
    \begin{itemize}
        \item a random subset of $m$ predictors is selected,
        \item the best split among these predictors is chosen using the Gini impurity
        criterion,
        \item the node is split into two child nodes.
    \end{itemize}
    Trees are grown without pruning until the minimum node size is reached.

    \item \textbf{Prediction aggregation} \\
    For a new observation $x$, each tree produces a predicted class probability.
    These probabilities are averaged across trees to obtain the Random Forest
    predicted default probability.
\end{enumerate}

\subsubsection{Model Evaluation}

Model performance is evaluated on a separate hold-out test sample obtained via
firm-level stratified sampling. For each observation in the test set, the Random Forest
produces a predicted default probability.

To obtain class labels, predicted probabilities are converted into default and
non-default outcomes using a fixed threshold of 0.5.

Because default events are rare, model performance is primarily assessed using the
area under the receiver operating characteristic curve (AUC). The AUC measures the
modelâ€™s ability to correctly rank defaulting firms above non-defaulting firms and is
independent of the chosen classification threshold. In addition, confusion matrices are
reported to summarize classification outcomes at the 0.5 threshold.

\subsubsection{Variable Importance}

To assess the contribution of individual predictors, variable importance measures
computed internally by the Random Forest algorithm are employed. In particular, we rely
primarily on permutation-based importance measures following \citet{breiman2001random}, which
quantify how strongly predictive performance deteriorates when the information contained
in a given predictor is destroyed.

For each predictor $X_j$, its values are randomly permuted among the out-of-bag (OOB)
observations, and the resulting decrease in predictive accuracy is recorded. Let
$\text{Acc}_{\text{OOB}}$ denote the original out-of-bag prediction accuracy and
$\text{Acc}_{\text{OOB}}^{(j,\text{perm})}$ the accuracy obtained after permuting $X_j$.
The permutation importance reported in the analysis corresponds to the absolute decrease
in accuracy,
\[
\text{VI}_j^{\text{abs}} =
\text{Acc}_{\text{OOB}} - \text{Acc}_{\text{OOB}}^{(j,\text{perm})}.
\]

Larger values of $\text{VI}_j^{\text{abs}}$ indicate that permuting predictor $X_j$ leads
to a substantial deterioration in out-of-bag predictive performance, implying a higher
contribution of the predictor to the Random Forest model. Values close to zero suggest
that the predictor has little influence on predictive accuracy.

In addition to permutation-based importance, the Random Forest algorithm also reports an
impurity-based importance measure, the mean decrease in Gini impurity. This measure
aggregates the total reduction in Gini impurity attributable to each predictor across
all splits and trees. While the mean decrease in Gini provides insight into how often a
variable is used to create purer nodes during tree construction, it is known to be
biased toward continuous predictors and predictors with many potential split points.

\citet{breiman2001random} originally expresses permutation importance as a percentage increase in
prediction error. The absolute formulation employed here is equivalent up to scaling and
preserves the same ranking of predictors. Consequently, permutation-based importance is
used as the primary criterion for ranking variables, while Gini-based importance is
reported for completeness.


\subsection{AdaBoost}

\subsubsection{Model description}

Empirical evidence on the performance of AdaBoost in credit-risk applications is mixed. While boosting methods are theoretically appealing, studies such as \citet{zhou2009credit} show that standard AdaBoost does not consistently outperform traditional models in highly imbalanced credit-scoring datasets. These findings suggest that AdaBoost can be sensitive to class imbalance and noise, both of which are characteristic features of default prediction data.

Nevertheless, AdaBoost remains a relevant benchmark in credit-risk modelling for several reasons. First, it is a well-established ensemble method that improves predictive performance by sequentially focusing on hard-to-classify observations, which is a central challenge in default prediction. Second, large-scale benchmarking studies \citep{lessmann2015benchmarking} emphasize that no single classifier dominates across datasets, making the inclusion of diverse model classes essential. Third, AdaBoost provides a useful contrast to bagging-based methods such as Random Forests, allowing the analysis to distinguish between bias-reduction mechanisms (boosting) and variance-reduction mechanisms (bagging).

Consistent with the data-splitting strategy described in Section 1.1.2, AdaBoost is trained exclusively on the firm-level training set. The adaptive reweighting of observations is therefore applied only within the training data and does not introduce information leakage across firms.

\subsubsection{Algorithm}

1.Initialization

All observations receive equal weights:
\[
w_i^{(1)} = \frac{1}{N}, \quad i = 1, \ldots, N.
\]

2.Iterative Boosting

For boosting iteration \( m = 1, \ldots, M \):

- A weak classifier \( h_m(x) \) is fitted using the current observation weights.

- The weighted classification error is computed as 
\[
\varepsilon_m = \sum_{i=1}^{N} w_i^{(m)} \mathbf{1}\{ h_m(x_i) \neq y_i \}. 
\]

- The classifier weight is given by

\[
\alpha_m = \frac{1}{2} \ln\left( \frac{1 - \varepsilon_m}{\varepsilon_m} \right).
\]

3.Weight Update

Observation weights are increased for misclassified observations:
\[
w_i^{(m+1)} = w_i^{(m)} \exp\left( \alpha_m \mathbf{1}\{ h_m(x_i) \neq y_i \} \right),
\]
and normalized such that
\[
\sum_{i=1}^{N} w_i^{(m+1)} = 1.
\]

4.Final Prediction

The AdaBoost classifier is a weighted combination of all weak learners:
\[
H(x) = \operatorname{sign}\left( \sum_{m=1}^{M} \alpha_m h_m(x) \right).
\]

The resulting score is mapped into a default probability, which is used for ranking firms by credit risk.

\subsubsection{Logic}

In the presence of strong class imblance, AdaBoost improves default prediction by sequentially increasing the importance of misclassified firms in the training set. Since default events are rare, they tend to be misclassified in early iterations and therefore receive higher weights in subsequent boosting rounds. This mechanism forces later weak learners to focus on economically fragile and borderline firms. Combined with firm-level stratified sampling, AdaBoost concentrates model capacity on difficult default cases while preserving a clean out-of-sample evaluation on unseen firms.

\subsection{Comments on code}

\textbf{Firm-Level Data Leakage Prevention}

Given the panel structure of the data, multiple observations may exist for the same firm across time. To ensure a valid out-of-sample evaluation, the data are split at the firm level, such that all observations of a given firm appear exclusively in either the training or the test set. 

A leakage check is performed prior to model estimation to verify that no firm identifier appears in both sample. Formally, letting $\mathcal{I}_{\text{train}}$ and $\mathcal{I}_{\text{test}}$ denote the sets of firm identifiers in the training and test data, respectively, the following condition is enforced:

\[
\mathcal{I}_{\mathrm{train}} \cap \mathcal{I}_{\mathrm{test}} = \emptyset.
\]

\textbf{Handling via Observation Weight}

The training data exhibits a strong class imbalance, with default events being rare relative to non-default observations. To address class imbalance, we assign observation-level weights based on inverse class frequencies:e.

Let $N$ denote the total number of training observations, and let $N_{+}$ and $N_{-}$ denote the number of default and non-default observations, respectively. Initial observation weights are defined as
\[
w_i =
\begin{cases}
\frac{N}{2N_{+}}, & \text{if } y_i = 1, \\
\frac{N}{2N_{-}}, & \text{if } y_i = 0.
\end{cases}
\]
This weighting scheme ensures that defaults and non-defaults contribute equally to the estimation objective, preventing the boosting algorithm from being dominated by the majority class in early iterations. Within the AdaBoost framework, initial observation weights enter directly into the weighted exponential loss function, implying that reweighting observations is equivalent to introducing asymmetric misclassification costs. This approach is well established in the literature on cost-sensitive and imbalanced classification (Friedman et al., 2000; Elkan, 2001; He and Garcia, 2009) and is commonly applied in credit-risk modeling where default events are rare but economically critical (Baesens et al., 2003; \citet{lessmann2015benchmarking}).


\subsection{XGBoost}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Start date for data}
\hlstd{start_date} \hlkwb{<-} \hlkwd{as.Date}\hlstd{(}\hlstr{"2024-12-31"}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}

\subsubsection{Model Description}

XGBoost (Extreme Gradient Boosting) is a scalable and highly efficient implementation of gradient boosted decision trees (GBDT). Unlike traditional Gradient Boosting Machines (GBM) that use first-order derivatives, XGBoost minimizes a regularized objective function using a second-order Taylor expansion, incorporating both the gradient (first derivative) and the hessian (second derivative). This allows for a more precise estimation of the loss reduction and faster convergence.

\subsubsection{Algorithm}

The XGBoost algorithm builds an ensemble of classification trees in an additive manner. At each iteration $t$, a new tree $f_t$ is added to the model to minimize the objective function.

\paragraph{Objective Function}
The objective function at iteration $t$ consists of a loss term and a regularization term:
\begin{equation}
\mathcal{L}^{(t)} = \sum_{i=1}^{n} l\left(y_i, \hat{y}_i^{(t-1)} + f_t(x_i)\right) + \Omega(f_t)
\end{equation}
where $l$ is the internal optimization metric, typically **Binary Logistic Loss (LogLoss)**, which measures the difference between the predicted probability and the true label $y_i$.

\paragraph{Taylor Expansion}
To efficiently optimize this objective, XGBoost employs a second-order Taylor expansion of the loss function:
\begin{equation}
\mathcal{L}^{(t)} \approx \sum_{i=1}^{n} \left[ l(y_i, \hat{y}^{(t-1)}) + g_i f_t(x_i) + \frac{1}{2} h_i f_t^2(x_i) \right] + \Omega(f_t)
\end{equation}
where $g_i = \partial_{\hat{y}^{(t-1)}} l(y_i, \hat{y}^{(t-1)})$ is the gradient (first derivative) and $h_i = \partial^2_{\hat{y}^{(t-1)}} l(y_i, \hat{y}^{(t-1)})$ is the hessian (second derivative).

\paragraph{Regularization}
The regularization term $\Omega(f_t)$ controls the complexity of the tree to prevent overfitting:
\begin{equation}
\Omega(f) = \gamma T + \frac{1}{2} \lambda ||w||^2
\end{equation}
Here, $T$ is the number of leaves, and $w$ represents the leaf weights. $\gamma$ penalizes the addition of new leaves (pruning), while $\lambda$ provides L2 smoothing of the leaf weights.

\subsubsection{Hyperparameter Specifications}

We optimize the following hyperparameters to control the bias-variance trade-off:

\begin{itemize}
    \item \textbf{Learning Rate ($\eta$):} Controls the step size shrinkage to prevent overfitting. Lower values require more trees but improve generalization.
    \item \textbf{Max Depth:} The maximum depth of a tree. Deeper trees capture more complex interactions but risk overfitting specific samples.
    \item \textbf{Subsample:} The fraction of observations randomly sampled for each tree. Values $<1.0$ introduce bagging-like randomness.
    \item \textbf{Colsample\_bytree:} The fraction of features randomly sampled for each tree. This decorrelates trees, similar to Random Forest.
\end{itemize}

\subsubsection{Tuning Strategies}

To maximize the \textbf{AUC (Area Under the Curve)} on the validation set, we compare three hyperparameter optimization strategies:

\begin{enumerate}
    \item \textbf{Discrete Grid Search:} A baseline that evaluates every combination of a fixed set of values. It is computationally expensive and limited by the pre-defined grid.
    \item \textbf{Random Search:} Samples hyperparameters from defined distributions. It is often more efficient than grid search for high-dimensional spaces as it explores a wider variety of values for continuous parameters like $\eta$.
    \item \textbf{Bayesian Optimization (TPE):} We employ the Tree-structured Parzen Estimator (TPE). Unlike random search, TPE builds a probabilistic model of the objective function (AUC) based on past evaluations to intelligently select the next hyperparameters. \citet{wang2019xgboost} demonstrate that TPE significantly improves stability and performance in credit scoring.
\end{enumerate}

\subsubsection{Potential Improvements}

A critical challenge in credit risk modeling is the severe class imbalance (defaults are rare). To address this, we propose an enhancement to the standard objective function.

Following the methodology of \citet{chang2018application}, we can calculate the \textit{imbalance ratio} (the ratio of non-default to default observations) and assign it to the \texttt{scale\_pos\_weight} parameter. This modifies the gradient calculation to penalize false negatives (missed defaults) more heavily than false positives. While our primary model uses standard LogLoss, implementing this weighted loss function is expected to improve the AUC by correcting the model's bias towards the majority class.

\subsection{CatBoost}

\section{Neural Networks}


%==== Chapter 3: Data =========================================================%

\chapter{Model Assessment}

\section{Final Model}


\section{Model Evaluation}






%==== Chapter 3: Task Distribution ============================================%

\chapter{Task Distribution}

The following matrix outlines the distribution of project responsibilities among the four team members. Primary ownership is denoted by an \textbf{X}.

\begin{table}[!h]
\centering
\caption{Team Task Distribution Matrix}
\label{tab:task_matrix}
\begin{tabular}{p{6cm} c c c c c}
\toprule
\textbf{Task} & \textbf{Implemented} &\textbf{Tristan} & \textbf{Nastia} & \textbf{Leonid} & \textbf{Martin} \\
\midrule
Data Preprocessing                   & Yes & \textbf{X} & \textbf{X} &            &            \\
Feature Engineering (Standardization \& PIT)     
                                     & Yes & \textbf{X} &            &            &            \\
Stratified Sampling within Train Set & No  &            &            &            & \textbf{X} \\
GLMs and regularized GLMs            & No  &            &            &            &            \\
Random Forest and Boosting           & No  & \textbf{X} & \textbf{X} &            &            \\
Neural Networks                      & No  &            &            &            &            \\
\bottomrule
\end{tabular}
\end{table}

%==== Chapter 4: Progress Bar =================================================%




%==== Chapter 7: References ===================================================%

\bibliographystyle{plainnat} 
\bibliography{references}

%==== Appendix ================================================================%

% \appendix 
% 
% \chapter{Overview}

%==== END =====================================================================%

\end{document}
