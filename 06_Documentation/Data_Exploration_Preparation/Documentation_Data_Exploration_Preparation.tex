%==== KNITR ===================================================================%



%==== START ===================================================================%

\documentclass{report}\usepackage[]{graphicx}\usepackage[]{xcolor}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}

\usepackage[left=2cm, right=2cm, top=1cm, bottom=2cm]{geometry}

% Font.


% Main packages.
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{booktabs} 
\usepackage{rotating} 
\usepackage{lmodern}

% Required for Table.


%%

\title{OeNB Industry Lab - Documentation}
\author{Tristan Leiter}
\date{\today}

%==== DOCUMENT START ==========================================================%

\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}

\maketitle

%==== ABSTRACT ================================================================%

% \begin{abstract}
% This report demonstrates the integration of R code and its output within a LaTeX document using Sweave. It covers the basic structure of a report, including a summary, chapters with subchapters, and a bibliography.
% \end{abstract}

%==== Table of content ========================================================%

\tableofcontents
\newpage

%==== Chapter 1: General ======================================================%

\chapter{Exploratory Data Analysis}

%==== Chapter 1a: Dependent Variable Analysis =================================%

\section{Dependent Variable Default}

In this credit risk dataset, the target variable, y, is inherently imbalanced. The number of non-defaults (0) significantly outnumbers the number of defaults (1). This is a common and expected characteristic of credit risk data.

This imbalance poses a significant challenge for model development. If we were to use a simple random split to create our training, validation, and test sets, we would face a high risk of creating unrepresentative samples. For example, a small test set could, purely by chance, end up with a much higher or lower percentage of defaults than the original dataset or, in the worst case, zero defaults.

\subsection{Data imbalance}

Below, we can observe the magnitude of the imbalance in the dataset. Table 1 shows
that less then 1\% of all observations in the dataset include defaults.

\begin{table}[h!]
\centering
\caption{Proportion of Defaulting vs. Non-Defaulting Firms}
\label{tab:imbalance-summary}
\begin{tabular}{lrr}
\toprule
\textbf{Default Status} & \textbf{Number of Firms} & \textbf{Proportion} \\
\midrule
1 (Default) & 2,096 & 0.008576 \\
0 (No Default) & 242,305 & 0.991424 \\
\midrule
\textbf{Total} & \textbf{244,401} & \textbf{1.000000} \\
\bottomrule
\end{tabular}
\end{table}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}
\includegraphics[width=\linewidth]{../../03_Charts/Data Exploration/01_Defaults_per_Sector} \caption[Defaulting Firms]{Defaulting Firms}\label{fig:default_sector_plot}
\end{figure}

\end{knitrout}

\clearpage

\subsection{Implication}

To address potential sampling bias from the severe data imbalance, we employ \textbf{stratified sampling}. This technique ensures that the original class distribution of the target variable is preserved in both the training and test sets.

The primary advantages of this approach are:

\begin{itemize}
    \item \textbf{Guaranteed Representation:} Stratification forces the data splits to maintain the original ratio of defaults to non-defaults. Given that 0.0865\% of the original dataset consists of defaults, both the training and test sets will contain approximately this same percentage.

    \item \textbf{Reliable Evaluation:} When the test set is truly representative, the performance metrics we calculate (such as accuracy, precision, recall, and F1-score) are meaningful. Evaluating a model on a test set with a skewed default rate would give a misleading and overly optimistic (or pessimistic) performance score.

    \item \textbf{Improved Model Generalization:} Training a model on a set that accurately reflects the real-world data distribution helps it learn the distinct patterns of both the majority (non-default) and minority (default) classes. This leads to a more robust and generalizable model.
\end{itemize}

In summary, using stratified sampling is a critical step to ensure our model is trained and evaluated on a reliable, representative foundation.

\subsection{Theory}

The dataset is partitioned into a training set and a testing set using the \texttt{rsample} package, which is part of the tidymodels framework. This process ensures that the resulting datasets are representative of the original data's class distribution, which is a critical step in handling data imbalance.

The used \texttt{initial\_split} function does not simply take a random 80\% of the data. When the \texttt{strata = y} argument is used, it follows a more intelligent \textbf{stratified sampling} algorithm:

\begin{enumerate}
    \item \textbf{Group Data by Stratum:} The function first identifies all unique values in the specified \texttt{strata} column (\texttt{y}). In this case, it creates two distinct groups: one for all the defaulting firms (\texttt{y = 1}) and another for all the non-defaulting firms (\texttt{y = 0}).

    \item \textbf{Sample Proportionally within Each Group:} The function then takes an 80\% sample from the defaulting firms group and a separate 80\% sample from the non-defaulting firms group.

    \item \textbf{Create the Training Set:} These two separately sampled subsets (80\% of defaults and 80\% of non-defaults) are combined to form the final training set (\texttt{Train}).

    \item \textbf{Create the Test Set:} The remaining 20\% of the defaulting firms and the remaining 20\% of the non-defaulting firms are combined to form the final testing set (\texttt{Test}).
\end{enumerate}

\clearpage

%==== Chapter 1b: Distributions & Bivariate Data Analysis =====================%

\section{Distributions and bivariate Data Analysis}

\subsection{Distributions}

\subsection{Data dependency}


%==== Chapter 1c: Feature Selection ===========================================%

\section{Feature selection}

Before building the predictive model, it is essential to assess the predictive power of each independent variable. This helps in eliminating irrelevant features, understanding variable relationships, and focusing on the most promising candidates. We use the Information Value (IV), significance tests and evaluation of multicollinearity for this purpose.

\subsection{Informational Value}

The function follows a systematic statistical procedure to assign a single score—the Information Value—to each variable, quantifying its ability to separate the "good" outcomes (non-defaults, y=0) from the "bad" ones (defaults, y=1).
Here is the step-by-step process:
\begin{enumerate}
\item \textbf{Binning (Grouping):} The function first discretizes each continuous independent variable into a set of bins or groups (e.g., by quantiles). For categorical variables, each category is treated as a separate group.

\item \textbf{Counting Goods and Bads:} For each bin of a variable (e.g., for the "Age: 20-30" bin), the algorithm counts:
\begin{itemize}
    \item The number of non-defaults.
    \item The number of defaults.
\end{itemize}

\item \textbf{Calculating Proportions:} It then calculates the proportion of the total goods and total bads that fall into that specific bin:
\begin{itemize}
    \item \textbf{\% Goods} = (\# Goods in the bin) / (Total \# Goods in the entire dataset)
    \item \textbf{\% Bads} = (\# Bads in the bin) / (Total \# Bads in the entire dataset)
\end{itemize}

\item \textbf{Calculating Weight of Evidence (WoE):} The WoE for each bin measures how much the evidence in that bin supports one outcome over the other. The formula is:
\[ \text{WoE} = \ln\left(\frac{\% \text{Goods}}{\% \text{Bads}}\right) \]
A large positive WoE means the bin is strongly associated with non-defaults, while a large negative WoE means it is strongly associated with defaults.

\item \textbf{Calculating Information Value (IV):} Finally, the total IV for the entire variable is calculated by summing a weighted value across all its bins. The formula is:
\[ \text{IV} = \sum (\% \text{Goods} - \% \text{Bads}) \times \text{WoE} \]
This single number represents the total predictive power of the variable.
\end{enumerate}
\subsubsection*{Why This is Useful Before Modeling}
Exploring the IV summary is not just a formality; it is a critical step in the exploratory data analysis (EDA) phase for several reasons:
\begin{itemize}
\item \textbf{Efficient Feature Selection:} IV provides a simple, powerful metric to rank all variables by their predictive strength. 

\begin{itemize}
\item \textbf{IV < 0.02:} Useless predictor.
\item \textbf{0.02 to 0.1:} Weak predictor.
\item \textbf{0.1 to 0.3:} Medium predictor.
\item \textbf{0.3 to 0.5:} Strong predictor.
\item \textbf{IV > 0.5:} Suspiciously high; may indicate data leakage or a variable that is too good to be true.
\end{itemize}

\item \textbf{Understanding Variable Relationships:} While the total IV gives a summary, the detailed WoE for each bin of a variable reveals the nature of its relationship with the outcome. For example, by looking at the WoE for different age groups, you can see if the default risk increases, decreases, or follows a non-linear pattern as age changes.

\item \textbf{Data Quality and Sanity Checks:} The IV calculation can expose data issues. A variable with an unexpectedly high IV might be a "leaky" variable (e.g., a variable that is only known *after* the default has occurred, pointing to a problematic quasi-seperation issue). Similarly, strange WoE patterns can highlight potential data entry errors or anomalies that need investigation.
\end{itemize}

\clearpage

\subsection{Signifiance Tests}

\subsection{Multicollinearity}

After evaluating the individual predictive power of each variable using Information Value, the next critical step is to examine the relationships between the predictor variables themselves. \textbf{Multicollinearity} occurs when two or more independent variables are highly correlated with each other.

While multicollinearity does not necessarily reduce the overall predictive accuracy of a model, it poses a significant problem for interpretation. When variables are highly correlated, it becomes difficult for the model to distinguish their individual impacts on the target variable. This can lead to:
\begin{itemize}
    \item Unstable coefficient estimates that can change dramatically with small changes in the data.
    \item High standard errors for the coefficients, making them seem statistically insignificant even when they are not.
    \item Difficulty in interpreting the model's logic, as the effect of one variable is confounded by the effect of another.
\end{itemize}

From the correlation matrix, several pairs of variables exhibit strong linear relationships (correlation > 0.70), which is a clear indicator of multicollinearity.

The analysis reveals two primary groups of highly inter-correlated variables:

\begin{enumerate}
    \item \textbf{Group 1: \texttt{f1}, \texttt{f2}, \texttt{f6}, and \texttt{f11}}
    \begin{itemize}
        \item The correlation between \texttt{f2} and \texttt{f1} is extremely high at \textbf{0.92}.
        \item \texttt{f6} is also highly correlated with both \texttt{f1} (0.79) and \texttt{f2} (0.74).
        \item \texttt{f11} is highly correlated with \texttt{f1} (0.74) and \texttt{f2} (0.66).
    \end{itemize}
    These variables likely measure a similar underlying economic or financial concept. Including all of them in a model would be redundant and problematic.

    \item \textbf{Group 2: \texttt{f8} and \texttt{f9}}
    \begin{itemize}
        \item The correlation between \texttt{f8} and \texttt{f9} is exceptionally high at \textbf{0.91}.
    \end{itemize}
    These two variables are nearly interchangeable from a linear perspective.
\end{enumerate}

Our strategy will be to select only one representative variable from each of these groups to proceed with modeling. The best candidate for selection is typically the variable with the highest Information Value (IV), as it holds the most individual predictive power.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}
\includegraphics[width=\linewidth]{../../03_Charts/Data Exploration/04_Multicollinearity} \caption[Multicollinearity]{Multicollinearity}\label{fig:mc_plot}
\end{figure}

\end{knitrout}

\clearpage

\subsection{Implication}

IV per feature:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}
\includegraphics[width=\linewidth]{../../03_Charts/Data Exploration/02_IV_per_feature} \caption[IV per feature]{IV per feature}\label{fig:iv_plot}
\end{figure}

\end{knitrout}

IV and multicollinearity:

\begin{table}[h!]
\centering
\caption{Variable Selection Summary: IV and Multicollinearity}
\label{tab:variable-selection}
\begin{tabular}{lccl}
\toprule
\textbf{Variable} & \textbf{Information Value (IV)} & \textbf{Multicollinearity Concern} & \textbf{Action} \\
\midrule
\multicolumn{4}{l}{\textit{Highly Correlated Group 1}} \\
f6                & 2.658                           & High (Group 1)                     & Keep (Highest IV in Group) \\
f11               & 2.630                           & High (Group 1)                     & Remove \\
f2                & 2.502                           & High (Group 1)                     & Remove \\
f1                & 2.280                           & High (Group 1)                     & Remove \\
\midrule
\multicolumn{4}{l}{\textit{Highly Correlated Group 2}} \\
f8                & 2.686                           & High (Group 2)                     & Keep (Highest IV in Group) \\
f9                & 2.670                           & High (Group 2)                     & Remove \\
\midrule
\multicolumn{4}{l}{\textit{Variables with Low Multicollinearity}} \\
f4                & 2.552                           & Low                                & Keep \\
f3                & 2.467                           & Low                                & Keep \\
f5                & 1.967                           & Low                                & Keep \\
f10               & 1.637                           & Low                                & Keep \\
f7                & 1.436                           & Low                                & Keep \\
public            & 0.549                           & Low                                & Keep \\
sector            & 0.079                           & Low                                & Keep \\
size              & 0.043                           & Low                                & Keep \\
\midrule
\multicolumn{4}{l}{\textit{Variable with Low Predictive Power}} \\
groupmember       & 0.002                           & Low                                & Remove (Useless IV) \\
\bottomrule
\end{tabular}
\end{table}```

\clearpage

%==== Chapter 1d: Implementation: Dividing the dataset ========================%

\section{Implementation}

\subsection{Data splitting}

In our methodology, we partition the data into an 80\% training set and a 20\% final hold-out test set. All model development, including hyperparameter tuning and feature selection, is performed on the 80\% training set using a technique called \textbf{k-fold cross-validation}. This approach is methodologically superior to a simpler three-way split (e.g., 50\% train, 25\% validation, 25\% test) for several critical reasons.

\subsubsection{Superiority Over a 50/25/25 Split}

\begin{enumerate}
    \item \textbf{Maximizes Data for Model Training:}
    The most significant drawback of a 50/25/25 split is that the model is only ever trained on 50\% of the available data. In contrast, with an 80/20 split and k-fold cross-validation, the model is repeatedly trained on a much larger portion of the data (e.g., in 5-fold CV, it trains on 80\% of the 80\% training set, which is 64\% of the total data, and this is done 5 times). More training data allows the model to learn more complex patterns and generalize better, reducing the risk of underfitting.

    \item \textbf{Robust and Stable Performance Estimation:}
    In a 50/25/25 split, model performance is evaluated and tuned based on a single, static 25\% validation set. The results on this single set can be highly variable depending on the "luck of the draw" of that particular split. A different random split could lead to different hyperparameter choices and a misleading performance estimate.

    Cross-validation solves this problem. By creating multiple (k) validation sets from the training data and averaging the performance metrics across all k folds, it provides a much more stable and reliable estimate of the model's true performance. It mitigates the risk of overfitting to a single validation set, leading to a more robust final model.

    \item \textbf{More Efficient Use of Data:}
    With cross-validation, every single observation in the 80\% training set is used as part of a validation set exactly once. This ensures that the entire training dataset is leveraged for both training and validation, making the process highly data-efficient. In a 50/25/25 split, the 25\% validation set is never used for training, and the 50\% training set is never used for validation, effectively "wasting" data from the perspective of each task.
\end{enumerate}

\subsubsection{Summary of Comparison}

The following table provides a direct comparison of the two approaches.

\begin{table}[h!]
\centering
\caption{Comparison of Data Splitting Strategies}
\label{tab:split-comparison}
\begin{tabular}{p{4cm}p{5cm}p{5cm}}
\toprule
\textbf{Aspect} & \textbf{80/20 Split with Cross-Validation} & \textbf{50/25/25 Train/Validation/Test Split} \\
\midrule
\textbf{Training Data Size} & Large (e.g., 64\%-72\% of total data used in each CV fold). & Small (fixed at 50\% of total data). \\
\midrule
\textbf{Performance Estimate Reliability} & \textbf{High.} Performance is averaged over multiple validation sets, leading to a stable and robust estimate. & \textbf{Low to Medium.} Performance depends on a single, potentially biased validation set. \\
\midrule
\textbf{Risk of Overfitting to Validation Set} & \textbf{Low.} It is difficult to overfit to multiple, distinct validation sets simultaneously. & \textbf{High.} Hyperparameters can be easily tuned to perform well on this specific 25\% set by chance. \\
\midrule
\textbf{Data Efficiency} & \textbf{Very High.} Every observation in the training set is used for both training and validation across the folds. & \textbf{Medium.} Data is strictly segregated; the validation set is never learned from. \\
\midrule
\textbf{Computational Cost} & Higher, as the model must be trained k times. & Lower, as the model is only trained once per hyperparameter set. \\
\bottomrule
\end{tabular}
\end{table}

In conclusion, while a simple three-way split is faster, the 80/20 split combined with cross-validation is the modern gold standard. It produces more robust, reliable, and better-performing models by using the available data more effectively, which is a trade-off well worth the additional computational cost.

%==== Chapter 1e: Outliers ====================================================%

\section{Outliers}

\subsection{Overview}


%==== Chapter 1f: Feature engineering =========================================%

\section{Feature engineering}

\subsection{Overview}


%==== Anhang ==================================================================%

\appendix 

\chapter{Overview}

%==== END =====================================================================%

\end{document}
