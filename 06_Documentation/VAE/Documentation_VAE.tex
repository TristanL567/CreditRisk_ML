%==== KNITR ===================================================================%



%==== START ===================================================================%

\documentclass{report}\usepackage[]{graphicx}\usepackage[]{xcolor}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}

\usepackage[left=2cm, right=2cm, top=1cm, bottom=2cm]{geometry}

% Language setting.
\usepackage[main=ngerman, provide=*]{babel}

% Main packages.
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{booktabs} 
\usepackage{rotating} 
\usepackage{lmodern}

% Citations.
\usepackage{natbib}
\setcitestyle{authoryear,open={(},close={)}} %Citation-related commands

%% Testing different layout.
\usepackage{geometry}
\usepackage{xcolor}

% Chart placement.
\usepackage{float}

% Grafik-Pfad.
% \graphicspath{
%     {O:/00_Procedures&Tasks/Anleitungen/05_Research/PerformanceMandate/Grafiken/}
% }

%==== DOCUMENT START ==========================================================%

\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}

\begin{titlepage}
    \newgeometry{top=3cm, bottom=3cm, left=2.5cm, right=2.5cm}
    \centering
    
    % \includegraphics[width=0.4\textwidth]{logo.png} \par
    {\scshape\LARGE ILAB: OeNB \par}
    \vspace{1cm}
    {\scshape\Large Documentation \par}
    
    \vspace{2.5cm}
    
    % --- Title Section ---
    \hrule height 2pt
    \vspace{0.5cm}
    { \huge \bfseries Variational Autoencoders \\}
    \vspace{0.5cm}
    \hrule height 2pt
    
    \vspace{2cm}
    
    % --- Author Section ---
    \Large
    \textbf{Author:} \\
    Tristan \textsc{Leiter}
    
    \vspace{1.5cm}
    
    % --- Metadata ---
    \normalsize
    % \textbf{Submission Date:} \\
    \today

    \vfill
    
\end{titlepage}
\restoregeometry

% \maketitle

%==== ABSTRACT ================================================================%

% \begin{abstract}
% This report demonstrates the integration of R code and its output within a LaTeX document using Sweave. It covers the basic structure of a report, including a summary, chapters with subchapters, and a bibliography.
% \end{abstract}

%==== Table of content ========================================================%

\tableofcontents
\newpage

%==== Chapter 1: Allgemein ====================================================%

%==== Chapter: VAE Integration and Diagnostics ================================%

\chapter{Hybrid VAE-XGBoost Architecture \& Implementation}

\section{Motivation and Objective}

The primary objective of this study is to augment a standard Gradient Boosting Machine (XGBoost) with unsupervised representation learning. While XGBoost is a powerful supervised learner, it relies on orthogonal decision boundaries that may fail to capture the complex, non-linear manifolds typical of distressed firms (e.g., "Zombie companies" that are technically insolvent but survive via liquidity).

We adopt a "Hybrid Expert" architecture where a Variational Autoencoder (VAE) and a Denoising Autoencoder (DAE) act as upstream feature extractors. These neural networks compress high-dimensional financial data into dense representations, which are then fed into the XGBoost model alongside the original features.

\section{Implementation Strategies}

The implementation iterates through four distinct feature engineering strategies (A through D), each targeting a specific type of risk signal.

\subsection{Strategy A: Latent Manifold Features (Standard VAE)}
This strategy utilizes the \textbf{Encoder} of a standard Variational Autoencoder to perform non-linear dimensionality reduction.

\begin{itemize}
    \item \textbf{Implementation:} The model compresses the 11 continuous financial ratios and one-hot encoded categorical metadata into an 8-dimensional probabilistic latent space.
    \item \textbf{Mechanism:} We extract the mean vector $\mu$ of the latent distribution $P(z|x)$ for each observation.
    \item \textbf{Code Logic:}
    \begin{itemize}
        \item The VAE is trained to minimize the Evidence Lower Bound (ELBO), balancing reconstruction loss against the KL-divergence from a standard normal prior.
        \item \texttt{Strategy\_A\_LF} = The 8-column matrix output from the bottleneck layer ($l_1 \dots l_8$).
    \end{itemize}
    \item \textbf{Hypothesis:} These features capture "clusters of behavior" (e.g., high-growth/low-liquidity) that raw ratios cannot express individually.
\end{itemize}

\subsection{Strategy B: Anomaly Scores (Reconstruction Error)}
This strategy utilizes the \textbf{Decoder} to quantify how "weird" a firm's financial structure is relative to the population.

\begin{itemize}
    \item \textbf{Implementation:} We measure the distance between the original input vector $x$ and the VAE's reconstruction $\hat{x}$. To handle mixed data types (continuous and categorical), the code implements a split-loss calculation.
    \item \textbf{Formula (Balanced Score):}
    \begin{equation}
        \text{Score} = w_{cont} \cdot \left( \frac{1}{N_{cont}} \sum (x_{cont} - \hat{x}_{cont})^2 \right) + w_{cat} \cdot \left( \frac{1}{N_{cat}} \text{BCE}(x_{cat}, \hat{x}_{cat}) \right)
    \end{equation}
    Where BCE is the Binary Cross Entropy for categorical variables.
    \item \textbf{Code Logic:}
    \begin{itemize}
        \item \texttt{anomaly\_score\_cont\_avg}: MSE normalized by the number of continuous columns.
        \item \texttt{anomaly\_score\_cat\_avg}: BCE normalized by the number of categorical columns (one-hot levels).
        \item The normalization prevents the score from being dominated by the categorical vector simply due to its higher dimensionality.
    \end{itemize}
    \item \textbf{Hypothesis:} High reconstruction error implies the firm's financial structure deviates from the norm. In credit risk, structural outliers are often highly correlated with default (e.g., fraud or extreme distress).
\end{itemize}

\subsection{Strategy C: Robust Features (Denoising Autoencoder)}

This strategy moves beyond the standard VAE by implementing a \textbf{Denoising Autoencoder (DAE)} to force feature robustness.

\begin{itemize}
    \item \textbf{Implementation:} Unlike Strategy A, this model is trained by intentionally corrupting the input data while forcing the model to predict the clean original data.
    \item \textbf{Mechanism:}
    \begin{itemize}
        \item \textbf{Noise Injection:} A Gaussian Noise layer ($\sigma = 0.1$) is added immediately after the input layer in Keras.
        \item \textbf{Training Objective:} Minimize $L(x, g(f(\tilde{x})))$, where $\tilde{x} = x + \epsilon$ and $\epsilon \sim \mathcal{N}(0, 0.1)$.
    \end{itemize}
    \item \textbf{Code Logic:}
    \begin{itemize}
        \item A separate Keras model (\texttt{dae\_autoencoder}) is defined using the Functional API.
        \item The encoder weights are extracted after training to generate features \texttt{dae\_l1} \dots \texttt{dae\_l8}.
    \end{itemize}
    \item \textbf{Hypothesis:} By forcing the network to "subtract" the noise, the latent features become invariant to small fluctuations and measurement errors in the financial ratios, focusing only on the robust structural signal.
\end{itemize}

\subsection{Strategy D: Domain Expert Interactions (The "Zombie" Detectors)}
This strategy eschews deep learning in favor of explicit domain knowledge, specifically targeting "Zombie Companies"—firms that are profitable but insolvent, or solvent but illiquid.

\begin{itemize}
    \item \textbf{Implementation:} We manually construct interaction terms that define diagonal decision boundaries XGBoost might struggle to approximate with shallow trees.
    \item \textbf{Key Features Created:}
    \begin{itemize}
        \item \textbf{Distress Gap (\texttt{Gap\_Debt\_Equity}):} $f_{11} - f_6$. This measures the absolute distance between Liabilities and Equity. A high positive gap indicates a leverage crisis regardless of firm size.
        \item \textbf{Cash Burn Ratio (\texttt{Ratio\_Cash\_Profit}):} $f_5 / (|f_8| + \epsilon)$. This identifies "Profitable but Illiquid" firms (False Negatives) where high accounting profit masks a dangerously low cash position.
        \item \textbf{Feature Stabilizer:} A conditional feature (\texttt{ifelse}) that swaps Profit for Cash when Profit is negative, creating a continuous "Solvency Capacity" metric.
    \end{itemize}
    \item \textbf{Hypothesis:} These ratios explicitly expose the "hidden buffers" that allow distressed firms to survive, directly addressing the "Healthy Loser" confusion matrix quadrant.
\end{itemize}

\section{Base Model Integration}

The final modeling stage aggregates the outputs of these strategies:
\begin{equation}
    X_{final} = [X_{raw}, X_{Latent(A)}, X_{Anomaly(B)}, X_{Robust(C)}, X_{Expert(D)}]
\end{equation}
This augmented dataset is fed into an XGBoost classifier. The inclusion of VAE/DAE features allows the gradient boosting model to view the data through multiple "lenses": the raw financial values, the probabilistic manifold (VAE), the robust structural view (DAE), and the financial analyst's view (Strategy D).

%==== Chapter 3: Final Model Optimization =====================================%

\chapter{Final Model Optimization: The Solvency Stabilizer}

\section{The Breakthrough: Solving the ``Healthy Loser'' Panic}

The primary weakness identified in previous iterations was the model's tendency to panic when observing negative profitability, resulting in a high rate of False Positives. This phenomenon, termed the ``Healthy Loser'' Panic, flagged firms that were technically unprofitable but structurally sound due to high cash reserves.

By implementing \textbf{Strategy D}, which utilizes the \texttt{Feature\_Stabilizer} (switching the focus to Cash $f5$ when Profit $f8$ is negative), we achieved a ``Double Win'': the model successfully caught more defaulters while drastically reducing false alarms.

\section{Quantitative Impact: Before vs. After}

The implementation of the stabilizer logic produced a decisive shift in model performance. Table \ref{tab:scorecard} details the impact on the confusion matrix.

\begin{table}[h]
\centering
\caption{Impact of Strategy D (Feature Stabilizer) on Model Performance}
\label{tab:scorecard}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Metric} & \textbf{Previous Result} & \textbf{Strategy D Result} & \textbf{Improvement} \\ \midrule
True Positives (Caught Risks) & 1,191 & \textbf{1,361} & +170 (Sensitivity $\uparrow$) \\
False Negatives (Missed Risks) & 275 & \textbf{105} & -170 (Safety $\uparrow$) \\
False Positives (False Alarms) & 39,738 & \textbf{26,077} & -13,661 (Precision $\uparrow$) \\
True Negatives (Correct Safe) & 129,450 & \textbf{143,111} & +13,661 (Efficiency $\uparrow$) \\ \bottomrule
\end{tabular}
\end{table}

\paragraph{Key Outcomes:}
\begin{itemize}
    \item \textbf{Efficiency:} 13,661 healthy firms were rescued from being wrongly flagged as risky.
    \item \textbf{Sensitivity:} Simultaneously, an additional 170 defaulters were caught that were previously missed.
    \item \textbf{Total Capture Rate:} The model now captures \textbf{92.8\%} of all defaults ($1,361 / 1,466$), a figure considered exceptional for credit risk modeling.
\end{itemize}

\section{Mechanism of Action: Why the Stabilizer Worked}

The significant drop in False Positives (from 39k to 26k) confirms the hypothesis regarding ``Healthy Losers.''

\begin{itemize}
    \item \textbf{The Old Logic:} The model observed $f8$ (Profit) at values like $-2.0$ and immediately classified the firm as high risk based on income statement bleeding.
    \item \textbf{The New Logic:} The \texttt{Feature\_Stabilizer} intervened: \textit{``Profit is negative; therefore, evaluate Cash ($f5$) instead.''}
    \item \textbf{The Result:} For the 13,661 rescued firms, the model recognized the ``Cash Cushion'' ($f5 > 0$) as a sufficient buffer against the ``Profit Bleed,'' correctly reclassifying them as Survivors (True Negatives).
\end{itemize}

\section{Forensic Analysis of Remaining Errors}

Despite optimization, specific error groups remain. A forensic analysis reveals the distinct profiles of these residual errors.

\subsection{The Remaining False Negatives (105 Firms)}
These are the \textbf{``Stealth Defaulters.''}
\begin{itemize}
    \item \textbf{Profile:} Median Profit ($f8$) $\approx -0.157$; Median Solvency Gap $\approx -0.235$.
    \item \textbf{Diagnosis:} These firms exhibit ``safe'' balance sheets (Equity $>$ Debt) and are barely losing money.
    \item \textbf{Root Cause:} Financially, they appear identical to safe firms. Their default is likely driven by non-financial factors—such as fraud, lawsuits, or sudden management exits—or extreme short-term liquidity shocks not captured in annual reports. This represents the likely ``irreducible error'' floor for a model based solely on annual financial statements.
\end{itemize}

\subsection{The Remaining False Positives (26,077 Firms)}
These are the \textbf{``Hardcore Zombies.''}
\begin{itemize}
    \item \textbf{Profile:} Massive Losses (Median $f8 \approx -1.05$); Massive Debt (Median Gap $\approx +0.90$).
    \item \textbf{Diagnosis:} By all standard financial logic, these firms should be insolvent.
    \item \textbf{Root Cause:} Their survival is likely due to external factors such as government bailouts, parent company guarantees, or extreme asset liquidation.
    \item \textbf{Interpretation:} Flagging these firms is the \textit{correct behavior} for a risk model. They represent high risk; they simply survived against the odds.
\end{itemize}

\section{Future Improvements and Limitations}

While the current model is production-ready, further reduction of the remaining error types requires data beyond the current scope.

\subsection{Addressing False Positives (The ``Zombie'' Survivor)}
The model fails to exclude these firms because their survival is mathematically improbable based on their financials alone.
\begin{itemize}
    \item \textbf{Limitation:} The model cannot see ``external support.''
    \item \textbf{Solution path:} To reduce these False Positives, we would need to integrate:
    \begin{enumerate}
        \item \textbf{Ownership Structure Data:} Identifying parent companies with deep pockets.
        \item \textbf{State Aid/Subsidy Data:} Identifying firms receiving government support.
        \item \textbf{News Sentiment:} Detecting announcements of restructuring or bailouts.
    \end{enumerate}
    \item \textbf{Recommendation:} Treat these 26,077 firms as a ``High Risk Watchlist.'' They are not defaults yet, but they are living on borrowed time.
\end{itemize}

\subsection{Addressing False Negatives (The ``Stealth'' Defaulter)}
The model fails to catch these firms because their annual reports look healthy right up until the moment of collapse.
\begin{itemize}
    \item \textbf{Limitation:} The latency of annual reporting masks sudden liquidity crises or fraud.
    \item \textbf{Solution path:} To capture these Stealth Defaulters, we would need:
    \begin{enumerate}
        \item \textbf{Higher Frequency Data:} Monthly cash flow or bank transaction data to catch sudden liquidity drying.
        \item \textbf{Fraud Detection Models:} Applying Benford’s Law or forensic accounting ratios to detect manipulated financial statements.
        \item \textbf{Legal Filings:} Monitoring court dockets for sudden litigation which often precedes ``healthy'' defaults.
    \end{enumerate}
\end{itemize}

\section{Final Recommendation}

We have reached the point of diminishing returns for feature engineering on this specific dataset. The recommendation is to \textbf{Stop Engineering} and accept the model. Capturing $\sim$93\% of defaults with a drastically reduced False Positive rate is a robust result. The pipeline (Strategy D + Threshold Optimization) should now be applied to the final Holdout Test Set to confirm these metrics on unseen data.

%==== Chapter 4: References ===================================================%

% \bibliographystyle{plainnat} 
% \bibliography{references}

%==== Appendix ================================================================%

% \appendix 
% 
% \chapter{Overview}

%==== END =====================================================================%

\end{document}
